# Roadmap

Roadmaps are a plan or strategy intended to achieve a particular purpose. 
They help one see where they are, where they are going, and how to get to their destination. 

Here is a road map for helping the Harold B. Lee Library transition from the current metadata formats to linked data. 

The milestones (or stages) in this roadmap are listed on the right.

I will now explain each milestone in this road map.

## Assessment

"In order to get where you want to go, you have to start out from where you are" (Merrill, 1989, p. 7).

The first milestone or stage is assessment. We need to assess where we are currently, where we have come from, what has been done, and what is currently being done as it relates to metadata in the library. 

This assessment stage includes diagramming and documenting the full lifecycle of a metadata record from birth to presentation; from creation or copy to the patron as exposed in ScholarSearch and from the main library website. 
This lifecycle captures the processes followed in all the systems we use (e.g. Symphony, ArchivesSpace, CONTENTdm) for creating, importing, maintaining, fixing, and converting records so that they can be used by patrons and staff for information retrieval. 
This lifecycle documentation will help give the library the full picture we need to be able to assess where we currently are and what is currently being done as it relates to metadata and linked data. 
It can help identify pain points in the process as well as duplication of effort. 
This can illuminate low hanging fruit that if updated will improve our existing processes and better prepare us for the transition to linked data.

This entire transition effort needs to be user-focused meaning that we focus on the needs of our users (i.e. students, faculty, BYU community, and staff). Our efforts must be in the service of our users to improve their ability to discover and retrive information.

To help assess and evaluate our success towards our patrons and staff we need to establish how we define, measure, and optimize for success. 
What are the quantitative measures for success?
In addition to quantitative measurements we should also consider how our relationships within the library between staff, departments, and divisions are improved through this transition. 
What if success is measured by whether our relationships improve with each other in the library as well as with students, faculty, and the rest of the campus community?
Are we a success if our efforts on this work leaves our community happier than before? (reference to Whats New podcast, episode 10)

Gathering requirements is another key step in this stage. 
What needs do we have in the library?
What are the needs of our users? 
Needs can be assessed by analyzing usage statistics (e.g. ScholarSearch stats tracker) for better understanding how our collections and items are being accessed and used. 
Conducting user tests is also another way to assess user needs.
This establishes a baseline of data to evaluate how well our efforts improve the experience of library patrons and staff. 

## Preparation

Our current metadata records exist in various systems (e.g. ArchivesSpace, CONTENTdm, and SirsiDynix Symphony) and have artifacts from past systems (i.e. RLIN and NOTIS) and past metadata content standards.
This stage is about cleaning, reconciling, and enriching our existing metadata to position us for conversion to linked data. 
The work we do in this stage would establish tools and processes that we could use repeatedly in the future as the task of maintaining metadata doesn't go away when moving to linked data. 

This work would include any existing efforts to upgrade metadata so the metadata is complete, correct, consistent, and avoids duplication.
Are there headings that aren't authorizing, data that lacks consistency, or duplicate records?

To prevent getting stuck in this stage 
We would need to identify and prioritize the upgrade tasks
We would need to determine the acceptable level of data quality that we want to achieve
What is the acceptable level of upgrade for our metadata?
What reports do we have that can help us identify data upgrade tasks?
Can we measure the number of types of metadata upgrading we need? 

Breaking this work down into categories or groups of metadata (i.e. item format, location, source) will help manage and organize the work to be done in this stage. 
We can start with smaller chunks of metadata to work on (e.g. the local name authorities) to learn best practices and gain experience that can be applied to larger sets of metadata.

Another task in this stage is to add URIs to existing entities. In the MARC records this is accomplished by using the 0 subfield. 
Work will need to be done to see how to best accomplish this step with other metadata formats. 
Work we do in this stage to reconcile entities and assign a URI will ease our transition when we convert to a linked data format. 
This reconcilliation step would utilize various reconciliation services to connect entities in our current metadata records with entities that already exist as linked open data. 
Some sources include LCNAF, VIAF, LCSH, Worldcat identities, Wikidata, and GeoNames.
Where we can't reconcile an entity in our metadata to an existing URI we will need to mint our own local URIs. 

The use of named-entity resolution (NER) is an area to explore to extract out data from descriptive metadata fields in an automated fashion. Extracting these entities helps identify and reconcile entities further in cases where a simply lookup is insufficient.

Our goal would be to automate this upgrade work as much as possible. 
Staff would also be involved in reviewing the upgraded metadata to make sure that any automated workflow is running correctly. 
Where we can't automate tasks successfully we would have staff intervene.

Choosing the best tools and approaches for each upgrade task will be crucial to assist in the automation and review of each upgrade task. 
Example tools and approaches to this work would include MarcEdit (Shieh & Reese, 2015), OpenRefine, programming scripts, and the use of a source control system that gives us the ability to see the history of changes we make to records, revert to previous versions of the metadata record, and see side-by-side diffs of what changes we make to a record. 

## Conversion

Once our metadata is upgraded we will positioned to convert our metadata to the linked data model.
This stage will lay the tools and processes for converting legacy data formats to linked data.

Creating crosswalks for MARC to other schemas like BIBFRAME and Schema.org + bib is an initial step in this stage.
Can we create a crosswalk from BIBFRAME to Schema.org?
We also need crosswalks from EAD (ArchivesSpace) to BIBFRAME and EAD to Schema.org as well as for Dublin Core (CONTENTdm).
These crosswalks enable us to convert to a linked data schema and back to the legacy format.

This includes determining what format(s) we want to target (e.g. BIBFRAME, Schema.org + bib, RDA + RDF, or our own schema and application profile, or multiple).

This stage will determine tools and workflows for converting the data. 
Similar to the previous stage this stage should include the use of source control to help track changes.

This conversion process would be done in parallel to existing systems. 
Metadata would continue to be created in existing formats (i.e. MARC, EAD, and DC) but would be converted into the RDF-based schema as an additional step in the process. 

The conversion would be done progressively. 
The conversion process needs to support converting a batch of records as well as converting a single record.
Similar to the approach in the metadata upgrade stage we would start with a smaller, discrete set of metadata to convert.
We can then work our way towards converting all metadata produced and maintained at the HBLL. 
Care would be taken after each conversion to ensure that we are getting the results we expect and need.
A way to check our conversion would be to convert the MARC to the RDF-based schema we are targeting and then convert that back to MARC. 
This second conversion will provide a way for us to do a quality control check on how well our conversion is working. 
The results of this second conversion can provide feedback to improve the conversion process. 

## Publication

Once we have the processes in place to upgrade and convert our metadata we can work on publishing our linked data on the World Wide Web.

There are various strategies we can follow in publishing our linked data (Shelby, 2017, p. 199-201). 
We can do a data dump (e.g. Library of Congress).
We can embed the structured data in existing HTML pages (as RDFa or JSON-LD).
We can provide a SPARQL endpoint.
We can provide APIs.
We can integrate with ScholarSearch to improve patron's ability to retrive information.
We can create new browsing interfaces for exploring and discovering the data that mashup metadata about entities we have in our local data with data from other sources like Wikidata.

Tim Berners-Lee talks about the five stars of linked data publishing (<https://www.w3.org/DesignIssues/LinkedData.html>).
Stars 1-4 are about making data available in increasingly more open and interoperable formats. 
The 5 stars of linked data publishing follows all the practices recommended in stars 1-4 and adds links to other data out on the World Wide Web. 
Publishing is about consuming as well.
We need to be a publisher and consumer of linked data.
We can also make this linked data available internally for other internal systems to consume and use. 

## Production

This stage is focused on establishing a production environment that creates metadata following a linked data model.
This environment includes the tools, systems, and workflows we use to generate metadata. 
We would try to use 3rd pary tools where they meet our needs and save us time and money. 
Where we can't find a 3rd party solution we would need to develop tools in-house. 

Characteristics of this cataloging environment would involve less typing and more lookup and reviewing. 
This is made possible by the change from strings to things. 
The system would provide helps where data entry is necessary (i.e. for dates or physical attributes of an item)
Overall the system would have each data element be strongly typed and would provide automated checks and verifications of data entered to help assist the cataloging process. 
Like previous stages the metadata created and maintained in this system would provide a source control mechanism that allows changes in a record to be tracked, diffed, and reverted. 

This production system would need to support data import and export to and from MARC as the transition away from MARC will be gradual. 
Work done in the metadata conversion stage would enable this ability. 

Work on this production system would be evolved over time. 
An event-drive architecture would give us flexibility in evolving and adapting this new production system to work with other library systems. 
It may be that we move from an integrated library system to a de-integrated library system where each library system uses standardized data formats and schemas that know how to communicate with each other.
To make this kind of message bus system work requires a common vocabulary for data to provide the needed interoperability. 

- Maxwell quotation about library systems

- data storage in ILS vs. data model
    - don't need to understand how the ILS is storing the data
    - how and what things can be said

## Evaluation

- reflection and lessons learned
- compare results to assessment outcomes/goals/objectives
- user tests
- did we accomplish what we set out to accomplish?
- did we improve things for the patron and staff?
- Our our relationships with each other in the library and with other interested parties and patrons better than we started?
- Our interested parties happier as an outcome of our efforts?

## Communication

Communication is essential to the success of this transition to linked data. 
The work that will be done to make this transition cannot be accomplished in back doors or by only one or two departments. 
It must be collaborative and inclusive. 
Transitioning current metadata formats to linked data will impact and involve multiple departments and divisions. 
This communication includes conveying the vision and direction of the road map at a macro level as well as working with the right people at each stage to ensure decisions are made with the correct understanding and involvement. 
We can't afford to make assumptions when deciding how to implement the road map without first verifying our assumptions with those that know.
At each stage we will need to make sure to involve the right people
The plans and work of each stage will need to be well documented and shared so that all who are interested can know the status and progress of the work being done. 

## Training

The next core stage in this road map is training. 
The training stage is meant to provide understanding and appreciation for the technology as well as the metadata and cataloging work being done by the library. 
It is not designed to transform catalogers into LIT staff or LIT staff into catalogers but to give a base level of understanding that can be shared among staff. 

Transitioning from the current metadata formats will require IT training for metadata and cataloging personnel. 
This includes learning about:

- web technologies like HTML, CSS, and JavaScript
- XML and XSLT
- different kinds of data models (i.e. flat, e-r, hierarchical, graph)
- linked data and the semantic web including, (but not limited to), RDF, SPARQL, and web ontologies
- Wikidata and Wikipedia
- OpenRefine (like Excel but on steroids)
- the use of the terminal and Python for data manipulation and scripting

There will also need to be metadata and cataloging training for LIT and other staff as needed.
This includes learning about:
- the art and science of cataloging
- authority work
- MARC records and their variation and nuance including discussion about serials, bound-with items, series, and electronic items
- cataloging tools such as MarcEdit, OCLC connexion, and SirsiDynix Symphony

This training can be accomplished in a variety of arenas:

- LIT and cataloging forums for library
- Regular tutorial training held monthly or (bi-weekly as necessary)
- Small group or one-on-one training for interested staff

The time commitment for this training would be offered on an as-needed basis and wouldn't take more than 10% of one's time each week. 

## Scholarship

The last stage is Scholarship. 
This refers to outreach and involvement within the scholarly community. 
It includes attending relevant conferences, presenting at conferences, and publishing in monographs and/or journals.

There is also opportunities to reach out to other institutions to learn from their experiences and receive feedback on our work. Some example institutions include

- Local academic institutions in the Mountain West (i.e. U of U, U of Nevada)
- other academic institutions doing linked data efforts (i.e. Stanford, U.C. Davis, Cornell, Harvard, University of Illinois at Urbana-Champaign)
- International institutions and academic libraries
- Library of Congress
- OCLC

This stage would also include joining and participating in professional or standard organizations and commitees. 

- BIBFRAME
- Schema.org and the bibliographic extension (https://bib.schema.org)
- OCLC Project Passage and other linked data-related OCLC efforts
- LD4P
- PCC
- NACO/SACO
- IFLA
- NISO
- RDA

# Ideas

- Maps to introduce the road map idea/thesis
- Alice in Wonderland and the cat to introduce why/destination/begin with the end in mind
